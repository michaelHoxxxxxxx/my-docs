# E2B LLM é›†æˆç¤ºä¾‹å¤§å…¨

æœ¬æ–‡æ¡£è¯¦ç»†ä»‹ç»äº† E2B ä¸å„ç§å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é›†æˆæ–¹æ³•å’Œç¤ºä¾‹ä»£ç ï¼Œæ¶µç›–ä¸»æµçš„ AI æœåŠ¡æä¾›å•†å’Œæœ€æ–°æ¨¡å‹ã€‚

## ğŸ“‹ ç›®å½•

- [OpenAI ç³»åˆ—é›†æˆ](#openai-ç³»åˆ—é›†æˆ)
- [Anthropic Claude é›†æˆ](#anthropic-claude-é›†æˆ)
- [å¼€æºæ¨¡å‹é›†æˆ](#å¼€æºæ¨¡å‹é›†æˆ)
- [ä¸“ä¸šæ¨¡å‹é›†æˆ](#ä¸“ä¸šæ¨¡å‹é›†æˆ)
- [ä¼ä¸šçº§å¹³å°é›†æˆ](#ä¼ä¸šçº§å¹³å°é›†æˆ)
- [æœ€ä½³å®è·µä¸æ€§èƒ½å¯¹æ¯”](#æœ€ä½³å®è·µä¸æ€§èƒ½å¯¹æ¯”)

## OpenAI ç³»åˆ—é›†æˆ

### 1. o1 å’Œ o3-mini æ¨¡å‹é›†æˆ

æœ€æ–°çš„æ¨ç†æ¨¡å‹ï¼Œç‰¹åˆ«é€‚åˆå¤æ‚çš„ä»£ç ç”Ÿæˆå’Œæ•°å­¦æ¨ç†ä»»åŠ¡ã€‚

#### Python ç¤ºä¾‹
```python
from e2b_code_interpreter import Sandbox
from openai import OpenAI
import json

client = OpenAI()

def analyze_data_with_o1(csv_path):
    """ä½¿ç”¨ o1 æ¨¡å‹è¿›è¡Œæ•°æ®åˆ†æå’Œå¯è§†åŒ–"""
    
    # åˆ›å»º E2B æ²™ç®±
    with Sandbox() as sandbox:
        # ä¸Šä¼ æ•°æ®æ–‡ä»¶
        with open(csv_path, "rb") as f:
            sandbox.files.write("/tmp/data.csv", f.read())
        
        # å‡†å¤‡åˆ†ææç¤º
        messages = [{
            "role": "user",
            "content": """
            åˆ†æ /tmp/data.csv æ–‡ä»¶ä¸­çš„æ•°æ®ï¼š
            1. è¯»å–å¹¶å±•ç¤ºæ•°æ®æ¦‚è§ˆ
            2. è¿›è¡Œç»Ÿè®¡åˆ†æ
            3. åˆ›å»ºå¯è§†åŒ–å›¾è¡¨
            4. ç”Ÿæˆæ´å¯ŸæŠ¥å‘Š
            """
        }]
        
        # è°ƒç”¨ o1 æ¨¡å‹
        response = client.chat.completions.create(
            model="o1-preview",  # æˆ– "o3-mini"
            messages=messages,
            tools=[{
                "type": "function",
                "function": {
                    "name": "execute_code",
                    "description": "åœ¨ E2B æ²™ç®±ä¸­æ‰§è¡Œ Python ä»£ç ",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "code": {
                                "type": "string",
                                "description": "è¦æ‰§è¡Œçš„ Python ä»£ç "
                            }
                        },
                        "required": ["code"]
                    }
                }
            }]
        )
        
        # å¤„ç†æ¨¡å‹å“åº”å¹¶æ‰§è¡Œä»£ç 
        for choice in response.choices:
            if hasattr(choice.message, 'tool_calls'):
                for tool_call in choice.message.tool_calls:
                    if tool_call.function.name == "execute_code":
                        args = json.loads(tool_call.function.arguments)
                        result = sandbox.run_code(args["code"])
                        print(f"æ‰§è¡Œç»“æœï¼š\n{result.logs.stdout}")
                        
                        # å¦‚æœç”Ÿæˆäº†å›¾è¡¨ï¼Œä¿å­˜å®ƒ
                        if result.artifacts:
                            for artifact in result.artifacts:
                                if artifact.type == "image":
                                    with open(f"output_{artifact.name}", "wb") as f:
                                        f.write(artifact.content)

# ä½¿ç”¨ç¤ºä¾‹
analyze_data_with_o1("sales_data.csv")
```

#### TypeScript ç¤ºä¾‹
```typescript
import { Sandbox } from '@e2b/code-interpreter'
import OpenAI from 'openai'

const openai = new OpenAI()

async function performMLAnalysis(datasetPath: string) {
  const sandbox = await Sandbox.create()
  
  try {
    // ä¸Šä¼ æ•°æ®é›†
    const data = await fs.readFile(datasetPath)
    await sandbox.files.write('/tmp/dataset.csv', data)
    
    // ä½¿ç”¨ o1 æ¨¡å‹è¿›è¡Œæœºå™¨å­¦ä¹ åˆ†æ
    const response = await openai.chat.completions.create({
      model: 'o1-preview',
      messages: [{
        role: 'user',
        content: `
          å¯¹ /tmp/dataset.csv æ‰§è¡Œå®Œæ•´çš„æœºå™¨å­¦ä¹ åˆ†æï¼š
          1. æ•°æ®é¢„å¤„ç†å’Œç‰¹å¾å·¥ç¨‹
          2. è®­ç»ƒå¤šä¸ªæ¨¡å‹ï¼ˆçº¿æ€§å›å½’ã€éšæœºæ£®æ—ã€XGBoostï¼‰
          3. äº¤å‰éªŒè¯å’Œæ¨¡å‹è¯„ä¼°
          4. ç‰¹å¾é‡è¦æ€§åˆ†æ
          5. ç”Ÿæˆé¢„æµ‹ç»“æœå’Œå¯è§†åŒ–
        `
      }],
      tools: [{
        type: 'function',
        function: {
          name: 'run_ml_code',
          description: 'æ‰§è¡Œæœºå™¨å­¦ä¹ ä»£ç ',
          parameters: {
            type: 'object',
            properties: {
              code: { type: 'string' }
            }
          }
        }
      }]
    })
    
    // å¤„ç†å¹¶æ‰§è¡Œç”Ÿæˆçš„ä»£ç 
    for (const choice of response.choices) {
      if (choice.message.tool_calls) {
        for (const toolCall of choice.message.tool_calls) {
          const args = JSON.parse(toolCall.function.arguments)
          const result = await sandbox.runCode(args.code)
          console.log('ML åˆ†æç»“æœ:', result.logs.stdout)
        }
      }
    }
  } finally {
    await sandbox.kill()
  }
}
```

### 2. GPT-4o å›¾åƒç†è§£é›†æˆ

GPT-4o æ”¯æŒå¤šæ¨¡æ€è¾“å…¥ï¼Œå¯ä»¥ç†è§£å’Œåˆ†æå›¾åƒæ•°æ®ã€‚

```python
from e2b_code_interpreter import Sandbox
from openai import OpenAI
import base64

def analyze_image_with_gpt4o(image_path):
    """ä½¿ç”¨ GPT-4o åˆ†æå›¾åƒå¹¶ç”Ÿæˆä»£ç """
    
    client = OpenAI()
    
    # è¯»å–å›¾åƒ
    with open(image_path, "rb") as f:
        image_data = base64.b64encode(f.read()).decode()
    
    with Sandbox() as sandbox:
        # å‡†å¤‡å¤šæ¨¡æ€æ¶ˆæ¯
        messages = [{
            "role": "user",
            "content": [
                {
                    "type": "text",
                    "text": "åˆ†æè¿™å¼ å›¾ç‰‡ï¼Œå¹¶ç¼–å†™ä»£ç æ¥é‡ç°å…¶ä¸­çš„æ•°æ®å¯è§†åŒ–æ•ˆæœ"
                },
                {
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:image/png;base64,{image_data}"
                    }
                }
            ]
        }]
        
        # è°ƒç”¨ GPT-4o
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=messages,
            max_tokens=2000
        )
        
        # æå–å¹¶æ‰§è¡Œç”Ÿæˆçš„ä»£ç 
        code = extract_code_from_response(response.choices[0].message.content)
        if code:
            result = sandbox.run_code(code)
            print("ä»£ç æ‰§è¡Œç»“æœï¼š", result.logs.stdout)
            
            # ä¿å­˜ç”Ÿæˆçš„å›¾è¡¨
            if result.artifacts:
                for artifact in result.artifacts:
                    if artifact.type == "image":
                        sandbox.download_file(artifact.path, f"recreated_{artifact.name}")

def extract_code_from_response(content):
    """ä»å“åº”ä¸­æå– Python ä»£ç å—"""
    import re
    code_pattern = r"```python\n(.*?)\n```"
    matches = re.findall(code_pattern, content, re.DOTALL)
    return "\n\n".join(matches) if matches else None
```

## Anthropic Claude é›†æˆ

### Claude 3 Opus ä»£ç è§£é‡Šå™¨

Claude 3 Opus åœ¨ä»£ç ç†è§£å’Œç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ã€‚

```python
from e2b_code_interpreter import Sandbox
from anthropic import Anthropic

def claude_code_interpreter(task_description):
    """ä½¿ç”¨ Claude 3 Opus ä½œä¸ºä»£ç è§£é‡Šå™¨"""
    
    anthropic = Anthropic()
    
    with Sandbox() as sandbox:
        # åˆ›å»ºç³»ç»Ÿæç¤º
        system_prompt = """
        ä½ æ˜¯ä¸€ä¸ªé«˜çº§ä»£ç è§£é‡Šå™¨ã€‚ä½ å¯ä»¥ï¼š
        1. ç¼–å†™å’Œæ‰§è¡Œ Python ä»£ç 
        2. åˆ†ææ•°æ®å’Œåˆ›å»ºå¯è§†åŒ–
        3. è§£å†³å¤æ‚çš„ç¼–ç¨‹é—®é¢˜
        4. ç”Ÿæˆé«˜è´¨é‡çš„ä»£ç æ–‡æ¡£
        
        å§‹ç»ˆä½¿ç”¨ execute_code å‡½æ•°æ¥è¿è¡Œä»£ç ã€‚
        """
        
        # ä¸ Claude äº¤äº’
        message = anthropic.messages.create(
            model="claude-3-opus-20240229",
            max_tokens=4000,
            system=system_prompt,
            messages=[{
                "role": "user",
                "content": task_description
            }],
            tools=[{
                "name": "execute_code",
                "description": "åœ¨æ²™ç®±ç¯å¢ƒä¸­æ‰§è¡Œ Python ä»£ç ",
                "input_schema": {
                    "type": "object",
                    "properties": {
                        "code": {
                            "type": "string",
                            "description": "è¦æ‰§è¡Œçš„ Python ä»£ç "
                        }
                    },
                    "required": ["code"]
                }
            }]
        )
        
        # å¤„ç† Claude çš„å·¥å…·è°ƒç”¨
        for content in message.content:
            if content.type == "tool_use":
                if content.name == "execute_code":
                    result = sandbox.run_code(content.input["code"])
                    print(f"æ‰§è¡Œç»“æœï¼š\n{result.logs.stdout}")
                    
                    # å¤„ç†é”™è¯¯
                    if result.error:
                        print(f"é”™è¯¯ï¼š{result.error}")
                    
                    # ä¿å­˜è¾“å‡ºæ–‡ä»¶
                    if result.artifacts:
                        for artifact in result.artifacts:
                            print(f"ç”Ÿæˆæ–‡ä»¶ï¼š{artifact.name}")

# ä½¿ç”¨ç¤ºä¾‹
claude_code_interpreter("""
åˆ›å»ºä¸€ä¸ªæœºå™¨å­¦ä¹ æ¨¡å‹æ¥é¢„æµ‹æˆ¿ä»·ï¼š
1. ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®é›†ï¼ˆ1000ä¸ªæ ·æœ¬ï¼‰
2. è¿›è¡Œç‰¹å¾å·¥ç¨‹
3. è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹
4. è¯„ä¼°æ¨¡å‹æ€§èƒ½
5. åˆ›å»ºç‰¹å¾é‡è¦æ€§å›¾è¡¨
""")
```

## å¼€æºæ¨¡å‹é›†æˆ

### 1. Llama 3.1 ç³»åˆ—é›†æˆ

é€šè¿‡ Fireworks AI æˆ– Together AI ä½¿ç”¨ Llama æ¨¡å‹ã€‚

```python
from e2b_code_interpreter import Sandbox
import requests

class LlamaCodeInterpreter:
    def __init__(self, api_key, provider="fireworks"):
        self.api_key = api_key
        self.provider = provider
        self.base_url = self._get_base_url()
    
    def _get_base_url(self):
        urls = {
            "fireworks": "https://api.fireworks.ai/inference/v1",
            "together": "https://api.together.xyz/v1"
        }
        return urls.get(self.provider)
    
    def execute_with_llama(self, prompt, model="llama-v3.1-405b-instruct"):
        """ä½¿ç”¨ Llama 3.1 ç”Ÿæˆå¹¶æ‰§è¡Œä»£ç """
        
        with Sandbox() as sandbox:
            # å‡†å¤‡è¯·æ±‚
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            }
            
            # æ„å»ºæç¤º
            system_prompt = """You are a code interpreter powered by Llama 3.1.
            When asked to perform tasks, write Python code to accomplish them.
            Always include proper error handling and comments."""
            
            data = {
                "model": f"accounts/fireworks/models/{model}",
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": prompt}
                ],
                "max_tokens": 2000,
                "temperature": 0.1,
                "tools": [{
                    "type": "function",
                    "function": {
                        "name": "run_python",
                        "description": "Execute Python code",
                        "parameters": {
                            "type": "object",
                            "properties": {
                                "code": {"type": "string"}
                            }
                        }
                    }
                }]
            }
            
            # å‘é€è¯·æ±‚
            response = requests.post(
                f"{self.base_url}/chat/completions",
                headers=headers,
                json=data
            )
            
            if response.status_code == 200:
                result = response.json()
                
                # æ‰§è¡Œç”Ÿæˆçš„ä»£ç 
                for choice in result.get("choices", []):
                    message = choice.get("message", {})
                    if "tool_calls" in message:
                        for tool_call in message["tool_calls"]:
                            if tool_call["function"]["name"] == "run_python":
                                code = json.loads(tool_call["function"]["arguments"])["code"]
                                exec_result = sandbox.run_code(code)
                                print(f"Llama 3.1 æ‰§è¡Œç»“æœï¼š\n{exec_result.logs.stdout}")
            
            return sandbox

# ä½¿ç”¨ç¤ºä¾‹
interpreter = LlamaCodeInterpreter(api_key="your_api_key")
interpreter.execute_with_llama("""
åˆ›å»ºä¸€ä¸ªå®æ—¶è‚¡ç¥¨ä»·æ ¼ç›‘æ§å™¨ï¼š
1. æ¨¡æ‹Ÿè‚¡ç¥¨ä»·æ ¼æ•°æ®
2. åˆ›å»ºä»·æ ¼èµ°åŠ¿å›¾
3. è®¡ç®—ç§»åŠ¨å¹³å‡çº¿
4. è¯†åˆ«ä¹°å…¥/å–å‡ºä¿¡å·
""")
```

### 2. Qwen2.5-Coder é›†æˆ

ä¸“é—¨é’ˆå¯¹ä»£ç ç”Ÿæˆä¼˜åŒ–çš„æ¨¡å‹ã€‚

```python
from e2b_code_interpreter import Sandbox
import openai

def qwen_coder_integration(task):
    """ä½¿ç”¨ Qwen2.5-Coder è¿›è¡Œä»£ç ç”Ÿæˆ"""
    
    # é…ç½® Fireworks AI
    client = openai.OpenAI(
        api_key="your_fireworks_api_key",
        base_url="https://api.fireworks.ai/inference/v1"
    )
    
    with Sandbox() as sandbox:
        # ä¸“é—¨çš„ç¼–ç æç¤º
        messages = [{
            "role": "system",
            "content": "You are Qwen2.5-Coder, specialized in generating high-quality, production-ready code."
        }, {
            "role": "user",
            "content": f"""
            Task: {task}
            
            Requirements:
            1. Write clean, well-documented code
            2. Include comprehensive error handling
            3. Follow best practices and design patterns
            4. Add unit tests where appropriate
            """
        }]
        
        # ç”Ÿæˆä»£ç 
        response = client.chat.completions.create(
            model="accounts/fireworks/models/qwen2p5-coder-32b-instruct",
            messages=messages,
            max_tokens=3000,
            temperature=0.1
        )
        
        # æå–å¹¶æ‰§è¡Œä»£ç 
        generated_code = response.choices[0].message.content
        
        # åˆ†ç¦»ä¸»ä»£ç å’Œæµ‹è¯•ä»£ç 
        main_code, test_code = separate_code_and_tests(generated_code)
        
        # æ‰§è¡Œä¸»ä»£ç 
        print("æ‰§è¡Œä¸»ä»£ç ...")
        main_result = sandbox.run_code(main_code)
        print(main_result.logs.stdout)
        
        # æ‰§è¡Œæµ‹è¯•
        if test_code:
            print("\næ‰§è¡Œå•å…ƒæµ‹è¯•...")
            test_result = sandbox.run_code(test_code)
            print(test_result.logs.stdout)
        
        return main_result, test_result

def separate_code_and_tests(code):
    """åˆ†ç¦»ä¸»ä»£ç å’Œæµ‹è¯•ä»£ç """
    # ç®€å•çš„åˆ†ç¦»é€»è¾‘ï¼Œå®é™…å¯ä»¥æ›´å¤æ‚
    if "def test_" in code or "import unittest" in code:
        parts = code.split("if __name__ == '__main__':")
        if len(parts) > 1:
            return parts[0], parts[1]
    return code, None
```

## ä¸“ä¸šæ¨¡å‹é›†æˆ

### 1. Mistral Codestral é›†æˆ

ä¸“é—¨çš„ä»£ç ç”Ÿæˆæ¨¡å‹ï¼Œæ”¯æŒå¤šç§ç¼–ç¨‹è¯­è¨€ã€‚

```typescript
import { Sandbox } from '@e2b/code-interpreter'
import { Mistral } from '@mistralai/mistralai'

class CodestralInterpreter {
  private mistral: Mistral
  private sandbox: Sandbox | null = null
  
  constructor(apiKey: string) {
    this.mistral = new Mistral({ apiKey })
  }
  
  async initialize() {
    this.sandbox = await Sandbox.create()
  }
  
  async generateAndExecute(prompt: string, language: string = 'python') {
    if (!this.sandbox) {
      throw new Error('Sandbox not initialized')
    }
    
    try {
      // ä½¿ç”¨ Codestral ç”Ÿæˆä»£ç 
      const response = await this.mistral.chat.complete({
        model: 'codestral-latest',
        messages: [{
          role: 'user',
          content: `Generate ${language} code for: ${prompt}\n\nRequirements:
          - Production-ready code
          - Proper error handling
          - Type hints (if applicable)
          - Docstrings
          - Follow ${language} best practices`
        }],
        temperature: 0.1,
        maxTokens: 2000
      })
      
      const generatedCode = response.choices[0].message.content
      
      // æ ¹æ®è¯­è¨€æ‰§è¡Œä»£ç 
      let result
      switch (language) {
        case 'python':
          result = await this.sandbox.runCode(generatedCode)
          break
        case 'javascript':
          result = await this.sandbox.runCode(generatedCode, { 
            language: 'javascript' 
          })
          break
        case 'typescript':
          // å…ˆç¼–è¯‘ TypeScript
          const jsCode = await this.compileTypeScript(generatedCode)
          result = await this.sandbox.runCode(jsCode, { 
            language: 'javascript' 
          })
          break
        default:
          throw new Error(`Unsupported language: ${language}`)
      }
      
      return {
        code: generatedCode,
        output: result.logs.stdout,
        error: result.error
      }
    } catch (error) {
      console.error('Codestral execution error:', error)
      throw error
    }
  }
  
  private async compileTypeScript(tsCode: string): Promise<string> {
    // åœ¨æ²™ç®±ä¸­ç¼–è¯‘ TypeScript
    const compileCode = `
import * as ts from 'typescript'

const code = \`${tsCode.replace(/`/g, '\\`')}\`
const result = ts.transpileModule(code, {
  compilerOptions: { 
    module: ts.ModuleKind.CommonJS,
    target: ts.ScriptTarget.ES2020
  }
})
console.log(result.outputText)
`
    const result = await this.sandbox!.runCode(compileCode)
    return result.logs.stdout
  }
  
  async cleanup() {
    if (this.sandbox) {
      await this.sandbox.kill()
    }
  }
}

// ä½¿ç”¨ç¤ºä¾‹
async function main() {
  const interpreter = new CodestralInterpreter('your_mistral_api_key')
  await interpreter.initialize()
  
  try {
    // Python ç¤ºä¾‹
    const pythonResult = await interpreter.generateAndExecute(
      'åˆ›å»ºä¸€ä¸ª REST API å®¢æˆ·ç«¯ç±»ï¼Œæ”¯æŒè®¤è¯å’Œé‡è¯•æœºåˆ¶',
      'python'
    )
    console.log('Python ä»£ç ï¼š', pythonResult.code)
    console.log('æ‰§è¡Œç»“æœï¼š', pythonResult.output)
    
    // TypeScript ç¤ºä¾‹
    const tsResult = await interpreter.generateAndExecute(
      'å®ç°ä¸€ä¸ªç±»å‹å®‰å…¨çš„äº‹ä»¶å‘å°„å™¨',
      'typescript'
    )
    console.log('TypeScript ä»£ç ï¼š', tsResult.code)
    
  } finally {
    await interpreter.cleanup()
  }
}
```

### 2. DeepSeek Coder é›†æˆ

é€šè¿‡ Together AI ä½¿ç”¨ DeepSeek Coderã€‚

```python
from e2b_code_interpreter import Sandbox
from typing import Dict, List, Optional
import httpx

class DeepSeekCoderIntegration:
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.base_url = "https://api.together.xyz/v1"
        
    async def analyze_codebase(self, repo_path: str) -> Dict:
        """ä½¿ç”¨ DeepSeek Coder åˆ†æä»£ç åº“"""
        
        async with httpx.AsyncClient() as client:
            with Sandbox() as sandbox:
                # ä¸Šä¼ ä»£ç åº“
                sandbox.upload_directory(repo_path, "/tmp/repo")
                
                # å‡†å¤‡åˆ†ææç¤º
                analysis_prompt = f"""
                Analyze the codebase at /tmp/repo and provide:
                1. Architecture overview
                2. Code quality assessment
                3. Security vulnerabilities
                4. Performance bottlenecks
                5. Refactoring suggestions
                
                Generate Python code to perform this analysis.
                """
                
                # è°ƒç”¨ DeepSeek Coder
                response = await client.post(
                    f"{self.base_url}/chat/completions",
                    headers={"Authorization": f"Bearer {self.api_key}"},
                    json={
                        "model": "deepseek-coder-33b-instruct",
                        "messages": [
                            {"role": "system", "content": "You are DeepSeek Coder, specialized in code analysis and generation."},
                            {"role": "user", "content": analysis_prompt}
                        ],
                        "max_tokens": 3000
                    }
                )
                
                if response.status_code == 200:
                    result = response.json()
                    analysis_code = result["choices"][0]["message"]["content"]
                    
                    # æ‰§è¡Œåˆ†æä»£ç 
                    analysis_result = sandbox.run_code(analysis_code)
                    
                    # ç”ŸæˆæŠ¥å‘Š
                    report_code = """
import json
import matplotlib.pyplot as plt
from datetime import datetime

# ç”Ÿæˆåˆ†ææŠ¥å‘Š
report = {
    "timestamp": datetime.now().isoformat(),
    "summary": analysis_result,
    "metrics": calculate_metrics(),
    "recommendations": generate_recommendations()
}

# åˆ›å»ºå¯è§†åŒ–
fig, axes = plt.subplots(2, 2, figsize=(12, 10))

# ä»£ç è´¨é‡åˆ†å¸ƒ
axes[0, 0].pie(quality_distribution, labels=quality_labels)
axes[0, 0].set_title('ä»£ç è´¨é‡åˆ†å¸ƒ')

# å¤æ‚åº¦çƒ­å›¾
axes[0, 1].imshow(complexity_heatmap, cmap='hot')
axes[0, 1].set_title('ä»£ç å¤æ‚åº¦çƒ­å›¾')

# ä¾èµ–å…³ç³»å›¾
axes[1, 0].scatter(dependency_x, dependency_y)
axes[1, 0].set_title('æ¨¡å—ä¾èµ–å…³ç³»')

# æŠ€æœ¯å€ºåŠ¡è¶‹åŠ¿
axes[1, 1].plot(tech_debt_timeline)
axes[1, 1].set_title('æŠ€æœ¯å€ºåŠ¡è¶‹åŠ¿')

plt.tight_layout()
plt.savefig('/tmp/codebase_analysis.png', dpi=300)

# ä¿å­˜æŠ¥å‘Š
with open('/tmp/analysis_report.json', 'w') as f:
    json.dump(report, f, indent=2)

print("åˆ†æå®Œæˆï¼æŠ¥å‘Šå·²ç”Ÿæˆã€‚")
"""
                    
                    sandbox.run_code(report_code)
                    
                    # ä¸‹è½½ç»“æœ
                    report = sandbox.download_file("/tmp/analysis_report.json")
                    visualization = sandbox.download_file("/tmp/codebase_analysis.png")
                    
                    return {
                        "report": report,
                        "visualization": visualization,
                        "logs": analysis_result.logs.stdout
                    }
```

## ä¼ä¸šçº§å¹³å°é›†æˆ

### 1. IBM WatsonX AI é›†æˆ

ä¼ä¸šçº§ AI å¹³å°é›†æˆç¤ºä¾‹ã€‚

```python
from e2b_code_interpreter import Sandbox
from ibm_watsonx_ai import APIClient
from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams
from ibm_watsonx_ai.foundation_models import ModelInference

class WatsonXE2BIntegration:
    def __init__(self, api_key: str, project_id: str):
        self.api_key = api_key
        self.project_id = project_id
        self.credentials = {
            "url": "https://us-south.ml.cloud.ibm.com",
            "apikey": api_key
        }
        self.client = APIClient(self.credentials)
        
    def create_enterprise_workflow(self, requirements: str):
        """åˆ›å»ºä¼ä¸šçº§å·¥ä½œæµ"""
        
        with Sandbox() as sandbox:
            # ä½¿ç”¨ IBM Granite æ¨¡å‹
            model = ModelInference(
                model_id="ibm/granite-13b-instruct-v2",
                credentials=self.credentials,
                project_id=self.project_id
            )
            
            # ç”Ÿæˆä¼ä¸šçº§ä»£ç 
            parameters = {
                GenParams.MAX_NEW_TOKENS: 2000,
                GenParams.TEMPERATURE: 0.1,
                GenParams.TOP_P: 0.9
            }
            
            prompt = f"""
            Create enterprise-grade Python code for: {requirements}
            
            Include:
            1. Comprehensive logging
            2. Error handling and recovery
            3. Configuration management
            4. Security best practices
            5. Performance monitoring
            6. Unit and integration tests
            """
            
            response = model.generate_text(
                prompt=prompt,
                params=parameters
            )
            
            # åœ¨æ²™ç®±ä¸­è®¾ç½®ä¼ä¸šç¯å¢ƒ
            setup_code = """
# è®¾ç½®ä¼ä¸šçº§ç¯å¢ƒ
import logging
import os
from datetime import datetime
import json

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('/tmp/enterprise_app.log'),
        logging.StreamHandler()
    ]
)

# åŠ è½½é…ç½®
config = {
    "environment": "production",
    "monitoring": {
        "enabled": True,
        "interval": 60
    },
    "security": {
        "encryption": "AES256",
        "authentication": "OAuth2"
    }
}

with open('/tmp/config.json', 'w') as f:
    json.dump(config, f)

print("ä¼ä¸šç¯å¢ƒé…ç½®å®Œæˆ")
"""
            
            sandbox.run_code(setup_code)
            
            # æ‰§è¡Œç”Ÿæˆçš„ä¼ä¸šä»£ç 
            result = sandbox.run_code(response)
            
            # è¿è¡Œæµ‹è¯•å¥—ä»¶
            test_code = """
import unittest
import sys

# åŠ¨æ€å¯¼å…¥ç”Ÿæˆçš„æ¨¡å—
sys.path.append('/tmp')

# è¿è¡Œæ‰€æœ‰æµ‹è¯•
loader = unittest.TestLoader()
suite = loader.discover('/tmp', pattern='test_*.py')
runner = unittest.TextTestRunner(verbosity=2)
result = runner.run(suite)

# ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
with open('/tmp/test_report.txt', 'w') as f:
    f.write(f"Tests run: {result.testsRun}\\n")
    f.write(f"Failures: {len(result.failures)}\\n")
    f.write(f"Errors: {len(result.errors)}\\n")
"""
            
            test_result = sandbox.run_code(test_code)
            
            # æ€§èƒ½åˆ†æ
            performance_code = """
import cProfile
import pstats
import io

# æ€§èƒ½åˆ†æ
profiler = cProfile.Profile()
profiler.enable()

# è¿è¡Œä¸»è¦åŠŸèƒ½
exec(open('/tmp/main.py').read())

profiler.disable()

# ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
s = io.StringIO()
ps = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
ps.print_stats(20)

with open('/tmp/performance_report.txt', 'w') as f:
    f.write(s.getvalue())

print("æ€§èƒ½åˆ†æå®Œæˆ")
"""
            
            sandbox.run_code(performance_code)
            
            # æ”¶é›†æ‰€æœ‰æŠ¥å‘Š
            reports = {
                "logs": sandbox.download_file("/tmp/enterprise_app.log"),
                "test_report": sandbox.download_file("/tmp/test_report.txt"),
                "performance_report": sandbox.download_file("/tmp/performance_report.txt"),
                "config": sandbox.download_file("/tmp/config.json")
            }
            
            return reports
```

### 2. Groq é«˜é€Ÿæ¨ç†é›†æˆ

ä½¿ç”¨ Groq çš„è¶…å¿«æ¨ç†é€Ÿåº¦è¿›è¡Œå®æ—¶ä»£ç ç”Ÿæˆã€‚

```typescript
import { Sandbox } from '@e2b/code-interpreter'
import Groq from 'groq-sdk'

class GroqRealtimeInterpreter {
  private groq: Groq
  private sandbox: Sandbox | null = null
  
  constructor(apiKey: string) {
    this.groq = new Groq({ apiKey })
  }
  
  async streamingCodeGeneration(userInput: string) {
    this.sandbox = await Sandbox.create()
    
    try {
      // ä½¿ç”¨ Groq çš„æµå¼å“åº”
      const stream = await this.groq.chat.completions.create({
        model: 'llama3-70b-8192',
        messages: [{
          role: 'user',
          content: `Generate Python code for: ${userInput}. 
                   Output executable code blocks that can be run immediately.`
        }],
        stream: true,
        temperature: 0.1,
        max_tokens: 2000
      })
      
      let currentCode = ''
      let codeBuffer = ''
      
      // å®æ—¶å¤„ç†æµå¼å“åº”
      for await (const chunk of stream) {
        const content = chunk.choices[0]?.delta?.content || ''
        currentCode += content
        
        // æ£€æµ‹å®Œæ•´çš„ä»£ç å—
        if (this.isCompleteCodeBlock(currentCode)) {
          const code = this.extractCode(currentCode)
          if (code && code !== codeBuffer) {
            codeBuffer = code
            
            // ç«‹å³æ‰§è¡Œä»£ç 
            console.log('æ‰§è¡Œä»£ç å—...')
            const result = await this.sandbox.runCode(code)
            
            // å®æ—¶æ˜¾ç¤ºç»“æœ
            if (result.logs.stdout) {
              console.log('è¾“å‡º:', result.logs.stdout)
            }
            if (result.error) {
              console.error('é”™è¯¯:', result.error)
            }
            
            // å¤„ç†ç”Ÿæˆçš„æ–‡ä»¶
            if (result.artifacts) {
              for (const artifact of result.artifacts) {
                console.log(`ç”Ÿæˆæ–‡ä»¶: ${artifact.name}`)
              }
            }
          }
        }
      }
      
      return currentCode
      
    } finally {
      if (this.sandbox) {
        await this.sandbox.kill()
      }
    }
  }
  
  private isCompleteCodeBlock(text: string): boolean {
    const codeBlockRegex = /```python\n[\s\S]*?\n```/g
    const matches = text.match(codeBlockRegex)
    return matches !== null && matches.length > 0
  }
  
  private extractCode(text: string): string | null {
    const codeBlockRegex = /```python\n([\s\S]*?)\n```/g
    const matches = [...text.matchAll(codeBlockRegex)]
    if (matches.length > 0) {
      return matches[matches.length - 1][1]
    }
    return null
  }
  
  async interactiveSession() {
    // åˆ›å»ºäº¤äº’å¼ä¼šè¯
    this.sandbox = await Sandbox.create()
    const sessionContext: string[] = []
    
    const processQuery = async (query: string) => {
      sessionContext.push(`User: ${query}`)
      
      const response = await this.groq.chat.completions.create({
        model: 'llama3-70b-8192',
        messages: [
          {
            role: 'system',
            content: 'You are an interactive Python code interpreter. Maintain context between queries.'
          },
          ...sessionContext.map(msg => ({
            role: 'user' as const,
            content: msg
          }))
        ],
        tools: [{
          type: 'function',
          function: {
            name: 'execute_python',
            description: 'Execute Python code in the sandbox',
            parameters: {
              type: 'object',
              properties: {
                code: { type: 'string' }
              }
            }
          }
        }]
      })
      
      // å¤„ç†å·¥å…·è°ƒç”¨
      const message = response.choices[0].message
      if (message.tool_calls) {
        for (const toolCall of message.tool_calls) {
          if (toolCall.function.name === 'execute_python') {
            const { code } = JSON.parse(toolCall.function.arguments)
            const result = await this.sandbox!.runCode(code)
            
            sessionContext.push(`Code executed: ${code}`)
            sessionContext.push(`Result: ${result.logs.stdout}`)
            
            return result
          }
        }
      }
      
      return null
    }
    
    return { processQuery, cleanup: () => this.sandbox?.kill() }
  }
}

// ä½¿ç”¨ç¤ºä¾‹
async function groqDemo() {
  const interpreter = new GroqRealtimeInterpreter('your_groq_api_key')
  
  // æµå¼ä»£ç ç”Ÿæˆ
  await interpreter.streamingCodeGeneration(
    'åˆ›å»ºä¸€ä¸ªå®æ—¶æ•°æ®ä»ªè¡¨æ¿ï¼Œæ˜¾ç¤º CPUã€å†…å­˜å’Œç£ç›˜ä½¿ç”¨æƒ…å†µ'
  )
  
  // äº¤äº’å¼ä¼šè¯
  const session = await interpreter.interactiveSession()
  
  await session.processQuery('åˆ›å»ºä¸€ä¸ªæ•°æ®æ¡†æ¶å¹¶åŠ è½½ç¤ºä¾‹æ•°æ®')
  await session.processQuery('å¯¹æ•°æ®è¿›è¡Œç»Ÿè®¡åˆ†æ')
  await session.processQuery('åˆ›å»ºå¯è§†åŒ–å›¾è¡¨')
  
  await session.cleanup()
}
```

## æœ€ä½³å®è·µä¸æ€§èƒ½å¯¹æ¯”

### æ¨¡å‹é€‰æ‹©æŒ‡å—

| æ¨¡å‹ | æœ€é€‚åˆåœºæ™¯ | å“åº”é€Ÿåº¦ | ä»£ç è´¨é‡ | æˆæœ¬ |
|------|-----------|---------|---------|------|
| o1/o3-mini | å¤æ‚æ¨ç†ã€ç®—æ³•è®¾è®¡ | æ…¢ | æé«˜ | é«˜ |
| GPT-4o | å¤šæ¨¡æ€ä»»åŠ¡ã€å›¾åƒç†è§£ | ä¸­ç­‰ | é«˜ | ä¸­é«˜ |
| Claude 3 Opus | é•¿æ–‡æœ¬ç†è§£ã€ä»£ç é‡æ„ | ä¸­ç­‰ | æé«˜ | é«˜ |
| Llama 3.1 405B | é€šç”¨ä»£ç ç”Ÿæˆ | å¿« | é«˜ | ä½ |
| Qwen2.5-Coder | ä¸“ä¸šä»£ç ç”Ÿæˆ | å¿« | é«˜ | ä½ |
| Codestral | å¤šè¯­è¨€ä»£ç ç”Ÿæˆ | å¿« | é«˜ | ä¸­ |
| DeepSeek Coder | ä»£ç åˆ†æã€ç†è§£ | å¿« | é«˜ | ä½ |
| Groq (Llama) | å®æ—¶äº¤äº’ã€ä½å»¶è¿Ÿ | æå¿« | ä¸­é«˜ | ä¸­ |

### é›†æˆæœ€ä½³å®è·µ

#### 1. é”™è¯¯å¤„ç†å’Œé‡è¯•
```python
import asyncio
from typing import Optional, Callable
from tenacity import retry, stop_after_attempt, wait_exponential

class RobustE2BIntegration:
    def __init__(self, primary_model: str, fallback_model: str):
        self.primary_model = primary_model
        self.fallback_model = fallback_model
    
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=4, max=10)
    )
    async def execute_with_retry(
        self, 
        code_generator: Callable,
        prompt: str
    ) -> dict:
        """å¸¦é‡è¯•æœºåˆ¶çš„ä»£ç æ‰§è¡Œ"""
        try:
            # å°è¯•ä½¿ç”¨ä¸»æ¨¡å‹
            result = await code_generator(self.primary_model, prompt)
            return result
        except Exception as e:
            print(f"ä¸»æ¨¡å‹å¤±è´¥: {e}, åˆ‡æ¢åˆ°å¤‡ç”¨æ¨¡å‹")
            # é™çº§åˆ°å¤‡ç”¨æ¨¡å‹
            result = await code_generator(self.fallback_model, prompt)
            return result
```

#### 2. æ²™ç®±èµ„æºä¼˜åŒ–
```python
from contextlib import asynccontextmanager
from typing import List
import asyncio

class SandboxPool:
    """æ²™ç®±æ± ç®¡ç†ï¼Œæé«˜æ€§èƒ½"""
    
    def __init__(self, pool_size: int = 5):
        self.pool_size = pool_size
        self.available_sandboxes: List[Sandbox] = []
        self.in_use_sandboxes: List[Sandbox] = []
        self._lock = asyncio.Lock()
    
    async def initialize(self):
        """é¢„åˆ›å»ºæ²™ç®±æ± """
        tasks = [Sandbox.create() for _ in range(self.pool_size)]
        self.available_sandboxes = await asyncio.gather(*tasks)
    
    @asynccontextmanager
    async def acquire(self):
        """è·å–æ²™ç®±"""
        async with self._lock:
            if not self.available_sandboxes:
                # å¦‚æœæ²¡æœ‰å¯ç”¨æ²™ç®±ï¼Œåˆ›å»ºæ–°çš„
                sandbox = await Sandbox.create()
            else:
                sandbox = self.available_sandboxes.pop()
            
            self.in_use_sandboxes.append(sandbox)
        
        try:
            yield sandbox
        finally:
            # æ¸…ç†å¹¶è¿”å›åˆ°æ± ä¸­
            await sandbox.reset()  # å‡è®¾æœ‰ reset æ–¹æ³•
            async with self._lock:
                self.in_use_sandboxes.remove(sandbox)
                self.available_sandboxes.append(sandbox)
    
    async def cleanup(self):
        """æ¸…ç†æ‰€æœ‰æ²™ç®±"""
        all_sandboxes = self.available_sandboxes + self.in_use_sandboxes
        tasks = [sandbox.kill() for sandbox in all_sandboxes]
        await asyncio.gather(*tasks)
```

#### 3. æ¨¡å‹å“åº”ç¼“å­˜
```python
import hashlib
import json
from typing import Optional
import aioredis

class ModelResponseCache:
    """ç¼“å­˜æ¨¡å‹å“åº”ä»¥æé«˜æ€§èƒ½"""
    
    def __init__(self, redis_url: str = "redis://localhost"):
        self.redis_url = redis_url
        self.redis: Optional[aioredis.Redis] = None
    
    async def connect(self):
        self.redis = await aioredis.from_url(self.redis_url)
    
    def _generate_cache_key(self, model: str, prompt: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        content = f"{model}:{prompt}"
        return f"e2b:cache:{hashlib.md5(content.encode()).hexdigest()}"
    
    async def get_cached_response(
        self, 
        model: str, 
        prompt: str
    ) -> Optional[str]:
        """è·å–ç¼“å­˜çš„å“åº”"""
        if not self.redis:
            return None
        
        key = self._generate_cache_key(model, prompt)
        cached = await self.redis.get(key)
        
        if cached:
            return json.loads(cached)
        return None
    
    async def cache_response(
        self, 
        model: str, 
        prompt: str, 
        response: str,
        ttl: int = 3600
    ):
        """ç¼“å­˜å“åº”"""
        if not self.redis:
            return
        
        key = self._generate_cache_key(model, prompt)
        await self.redis.setex(
            key, 
            ttl, 
            json.dumps(response)
        )
```

### å®‰å…¨æ€§æœ€ä½³å®è·µ

```python
import os
import secrets
from cryptography.fernet import Fernet

class SecureE2BIntegration:
    """å®‰å…¨çš„ E2B é›†æˆ"""
    
    def __init__(self):
        # ç”ŸæˆåŠ å¯†å¯†é’¥
        self.encryption_key = Fernet.generate_key()
        self.cipher = Fernet(self.encryption_key)
    
    def sanitize_code(self, code: str) -> str:
        """æ¸…ç†æ½œåœ¨çš„æ¶æ„ä»£ç """
        # ç§»é™¤å±é™©çš„å¯¼å…¥
        dangerous_imports = [
            'os.system', 'subprocess', 'eval', 'exec',
            '__import__', 'compile', 'open'
        ]
        
        for dangerous in dangerous_imports:
            if dangerous in code:
                raise ValueError(f"å±é™©æ“ä½œæ£€æµ‹: {dangerous}")
        
        return code
    
    def encrypt_sensitive_data(self, data: str) -> bytes:
        """åŠ å¯†æ•æ„Ÿæ•°æ®"""
        return self.cipher.encrypt(data.encode())
    
    def create_secure_sandbox(self) -> Sandbox:
        """åˆ›å»ºå®‰å…¨é…ç½®çš„æ²™ç®±"""
        return Sandbox.create(
            # é™åˆ¶èµ„æº
            cpu_limit=1.0,  # 1 ä¸ª CPU æ ¸å¿ƒ
            memory_limit="512MB",
            timeout=30000,  # 30 ç§’è¶…æ—¶
            
            # ç½‘ç»œéš”ç¦»
            network_enabled=False,
            
            # æ–‡ä»¶ç³»ç»Ÿé™åˆ¶
            max_file_size="10MB",
            
            # ç¯å¢ƒå˜é‡ç™½åå•
            env_whitelist=["PATH", "PYTHONPATH"]
        )
```

## æ€»ç»“

æœ¬æ–‡æ¡£è¯¦ç»†ä»‹ç»äº† E2B ä¸å„ç§ LLM çš„é›†æˆæ–¹æ³•ï¼Œä»æœ€æ–°çš„ OpenAI o1/o3-mini æ¨¡å‹åˆ°å¼€æºçš„ Llama ç³»åˆ—ï¼Œå†åˆ°ä¼ä¸šçº§çš„ WatsonX å¹³å°ã€‚æ¯ç§é›†æˆéƒ½æœ‰å…¶ç‹¬ç‰¹çš„ä¼˜åŠ¿ï¼š

- **OpenAI ç³»åˆ—**ï¼šé€‚åˆéœ€è¦é«˜è´¨é‡æ¨ç†å’Œå¤šæ¨¡æ€èƒ½åŠ›çš„åœºæ™¯
- **Claude ç³»åˆ—**ï¼šåœ¨ä»£ç ç†è§£å’Œé•¿æ–‡æœ¬å¤„ç†æ–¹é¢è¡¨ç°å“è¶Š
- **å¼€æºæ¨¡å‹**ï¼šæä¾›äº†æˆæœ¬æ•ˆç›Šé«˜çš„è§£å†³æ–¹æ¡ˆ
- **ä¸“ä¸šæ¨¡å‹**ï¼šé’ˆå¯¹ç‰¹å®šç¼–ç¨‹ä»»åŠ¡ä¼˜åŒ–
- **ä¼ä¸šå¹³å°**ï¼šæä¾›å®Œæ•´çš„ä¼ä¸šçº§åŠŸèƒ½å’Œæ”¯æŒ

é€‰æ‹©åˆé€‚çš„æ¨¡å‹éœ€è¦è€ƒè™‘ä»»åŠ¡å¤æ‚åº¦ã€å“åº”æ—¶é—´è¦æ±‚ã€æˆæœ¬é¢„ç®—å’Œå®‰å…¨æ€§éœ€æ±‚ã€‚é€šè¿‡æœ¬æ–‡æ¡£æä¾›çš„ç¤ºä¾‹å’Œæœ€ä½³å®è·µï¼Œä½ å¯ä»¥å¿«é€Ÿå®ç°å„ç§ LLM ä¸ E2B çš„é›†æˆï¼Œæ„å»ºå¼ºå¤§çš„ AI ä»£ç è§£é‡Šå™¨åº”ç”¨ã€‚

## ç›¸å…³èµ„æº

- [E2B å®˜æ–¹æ–‡æ¡£](https://e2b.dev/docs)
- [E2B Cookbook](https://github.com/e2b-dev/e2b-cookbook)
- [å„ LLM æä¾›å•†æ–‡æ¡£](#)
- [æ€§èƒ½åŸºå‡†æµ‹è¯•ç»“æœ](#)